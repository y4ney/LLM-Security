# LLM Security 论文资料收集

## 2024-04

6. JailbreakLens：针对大型语言模型的越狱攻击可视化分析

- 📅 日期：2024-04-12
- 📑 文件：[JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models](./JailbreakLens-%20Visual%20Analysis%20of%20Jailbreak%20Attacks%20Against%20Large%20Language%20Models.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.08793>

5. 次毒性问题：深入探讨大型语言模型在越狱尝试中响应态度的变化

- 📅 日期：2024-04-12
- 📑 文件：[Subtoxic Questions: Dive Into Attitude Change of LLM's Response in Jailbreak Attempts](./Subtoxic%20Questions-%20Dive%20Into%20Attitude%20Change%20of%20LLM's%20Response%20in%20Jailbreak%20Attempts.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.08309>

4. 目标引导的生成式提示注入攻击在大型语言模型上的应用

- 📅 日期：2024-04-06
- 📑 文件：[Goal-guided Generative Prompt Injection Attack on Large Language Models](./Goal-guided%20Generative%20Prompt%20Injection%20Attack%20on%20Large%20Language%20Models.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.07234>

3. 微调和量化增加了大型语言模型的漏洞

- 📅 日期：2024-04-05
- 📑 文件：[Increased LLM Vulnerabilities from Fine-tuning and Quantization](./Increased%20LLM%20Vulnerabilities%20from%20Fine-tuning%20and%20Quantization.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.04392>

2. 越狱提示攻击：一种可控的对扩散模型的对抗性攻击

- 📅 日期：2024-04-02
- 📑 文件：[Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models](./Jailbreaking%20Prompt%20Attack-%20A%20Controllable%20Adversarial%20Attack%20against%20Diffusion%20Models.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.02928>

## 2023-11

1. 召唤恶魔并将其束缚：野外LLM红队攻击的实地理论

- 📅 日期：2023-11-10（最后修改日期：2023-11-13）
- 📑 文件：[Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild](./Summon%20a%20Demon%20and%20Bind%20it:%20A%20Grounded%20Theory%20of%20LLM%20Red%20Teaming%20in%20the%20Wild.pdf)
- 🔗 链接：<https://arxiv.org/abs/2311.06237>
