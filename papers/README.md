# LLM Security 论文资料收集

## 2024-04

9. 介绍来自MLCommons的人工智能安全基准v0.5

- 📅 日期：2024-04-18
- 📑 文件：[Introducing v0.5 of the AI Safety Benchmark from MLCommons](./Introducing%20v0.5%20of%20the%20AI%20Safety%20Benchmark%20from%20MLCommons.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.12241>


8. JailbreakLens：针对大型语言模型的越狱攻击可视化分析

- 📅 日期：2024-04-12
- 📑 文件：[JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models](./JailbreakLens-%20Visual%20Analysis%20of%20Jailbreak%20Attacks%20Against%20Large%20Language%20Models.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.08793>

7. 次毒性问题：深入探讨大型语言模型在越狱尝试中响应态度的变化

- 📅 日期：2024-04-12
- 📑 文件：[Subtoxic Questions: Dive Into Attitude Change of LLM's Response in Jailbreak Attempts](./Subtoxic%20Questions-%20Dive%20Into%20Attitude%20Change%20of%20LLM's%20Response%20in%20Jailbreak%20Attempts.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.08309>

6. AEGIS：使用大型语言模型专家团队进行在线自适应人工智能内容安全审核

- 📅 日期：2024-04-09
- 📑 文件：[AEGIS- Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts](./AEGIS-%20Online%20Adaptive%20AI%20Content%20Safety%20Moderation%20with%20Ensemble%20of%20LLM%20Experts.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.05993>

5. 目标引导的生成式提示注入攻击在大型语言模型上的应用

- 📅 日期：2024-04-06
- 📑 文件：[Goal-guided Generative Prompt Injection Attack on Large Language Models](./Goal-guided%20Generative%20Prompt%20Injection%20Attack%20on%20Large%20Language%20Models.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.07234>

4. 微调和量化增加了大型语言模型的漏洞

- 📅 日期：2024-04-05
- 📑 文件：[Increased LLM Vulnerabilities from Fine-tuning and Quantization](./Increased%20LLM%20Vulnerabilities%20from%20Fine-tuning%20and%20Quantization.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.04392>

3. 越狱提示攻击：一种可控的对扩散模型的对抗性攻击

- 📅 日期：2024-04-02
- 📑 文件：[Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models](./Jailbreaking%20Prompt%20Attack-%20A%20Controllable%20Adversarial%20Attack%20against%20Diffusion%20Models.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.02928>

## 2023-11

2. 召唤恶魔并将其束缚：野外LLM红队攻击的实地理论

- 📅 日期：2023-11-10（最后修改日期：2023-11-13）
- 📑 文件：[Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild](./Summon%20a%20Demon%20and%20Bind%20it:%20A%20Grounded%20Theory%20of%20LLM%20Red%20Teaming%20in%20the%20Wild.pdf)
- 🔗 链接：<https://arxiv.org/abs/2311.06237>

## 2022-11

1. 忽略之前的提示：语言模型的攻击技术

- 📅 日期：2022-11-17
- 📑 文件：[Ignore Previous Prompt: Attack Techniques For Language Models](./Ignore%20Previous%20Prompt-%20Attack%20Techniques%20For%20Language%20Models.pdf)
- 🔗 链接：<https://arxiv.org/abs/2211.09527>
