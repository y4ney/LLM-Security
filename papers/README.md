# LLM Security 论文资料收集

## 2024-04

13. 迭代提示多模态LLM以复制自然和AI生成的图像

- 📅 日期：2024-04-21
- 📑 文件：[Iteratively Prompting Multimodal LLMs to Reproduce Natural and AI-Generated Images](./Iteratively%20Prompting%20Multimodal%20LLMs%20to%20Reproduce%20Natural%20and%20AI-Generated%20Images.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.13784>

12. 大语言模型中的错误标记：分类法与有效检测方法

- 📅 日期：2024-04-19
- 📑 文件：[Glitch Tokens in Large Language Models: Categorization
Taxonomy and Effective Detection](./Glitch%20Tokens%20in%20Large%20Language%20Models-%20Categorization%20Taxonomy%20and%20Effective%20Detection.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.09894>

11. 指令层次结构：训练大语言模型优先处理特权指令

- 📅 日期：2024-04-19
- 📑 文件：[The Instruction Hierarchy:Training LLMs to Prioritize Privileged Instructions](./The%20Instruction%20Hierarchy-%20Training%20LLMs%20to%20Prioritize%20Privileged%20Instructions.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.13208>

10. 介绍来自MLCommons的人工智能安全基准v0.5

- 📅 日期：2024-04-18
- 📑 文件：[Introducing v0.5 of the AI Safety Benchmark from MLCommons](./Introducing%20v0.5%20of%20the%20AI%20Safety%20Benchmark%20from%20MLCommons.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.12241>


9. JailbreakLens：针对大型语言模型的越狱攻击可视化分析

- 📅 日期：2024-04-12
- 📑 文件：[JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models](./JailbreakLens-%20Visual%20Analysis%20of%20Jailbreak%20Attacks%20Against%20Large%20Language%20Models.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.08793>

8. 次毒性问题：深入探讨大型语言模型在越狱尝试中响应态度的变化

- 📅 日期：2024-04-12
- 📑 文件：[Subtoxic Questions: Dive Into Attitude Change of LLM's Response in Jailbreak Attempts](./Subtoxic%20Questions-%20Dive%20Into%20Attitude%20Change%20of%20LLM's%20Response%20in%20Jailbreak%20Attempts.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.08309>

7. AEGIS：使用大型语言模型专家团队进行在线自适应人工智能内容安全审核

- 📅 日期：2024-04-09
- 📑 文件：[AEGIS- Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts](./AEGIS-%20Online%20Adaptive%20AI%20Content%20Safety%20Moderation%20with%20Ensemble%20of%20LLM%20Experts.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.05993>

6. 目标引导的生成式提示注入攻击在大型语言模型上的应用

- 📅 日期：2024-04-06
- 📑 文件：[Goal-guided Generative Prompt Injection Attack on Large Language Models](./Goal-guided%20Generative%20Prompt%20Injection%20Attack%20on%20Large%20Language%20Models.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.07234>

5. 微调和量化增加了大型语言模型的漏洞

- 📅 日期：2024-04-05
- 📑 文件：[Increased LLM Vulnerabilities from Fine-tuning and Quantization](./Increased%20LLM%20Vulnerabilities%20from%20Fine-tuning%20and%20Quantization.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.04392>

4. 越狱提示攻击：一种可控的对扩散模型的对抗性攻击

- 📅 日期：2024-04-02
- 📑 文件：[Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models](./Jailbreaking%20Prompt%20Attack-%20A%20Controllable%20Adversarial%20Attack%20against%20Diffusion%20Models.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.02928>

## 2024-01

3. OWASP 大语言模型人工智能应用Top 10 安全威胁

- 📅 日期：2024-01-09
- 📑 文件：[OWASP 大语言模型人工智能应用Top 10 安全威胁](./OWASP%20大语言模型人工智能应用Top%2010%20安全威胁.pdf)
- 🔗 链接：<https://owasp.org/www-project-top-10-for-large-language-model-applications/>

## 2023-11

2. 召唤恶魔并将其束缚：野外LLM红队攻击的实地理论

- 📅 日期：2023-11-10（最后修改日期：2023-11-13）
- 📑 文件：[Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild](./Summon%20a%20Demon%20and%20Bind%20it:%20A%20Grounded%20Theory%20of%20LLM%20Red%20Teaming%20in%20the%20Wild.pdf)
- 🔗 链接：<https://arxiv.org/abs/2311.06237>

## 2022-11

1. 忽略之前的提示：语言模型的攻击技术

- 📅 日期：2022-11-17
- 📑 文件：[Ignore Previous Prompt: Attack Techniques For Language Models](./Ignore%20Previous%20Prompt-%20Attack%20Techniques%20For%20Language%20Models.pdf)
- 🔗 链接：<https://arxiv.org/abs/2211.09527>
