# LLM Security 论文资料收集

## 2024-05

18. 大型语言模型能深入检测复杂的恶意查询吗？一个通过混淆意图实现越狱的框架

- 📅 日期：2024-05-06
- 📑 文件：[Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent](./CAN%20LLMS%20DEEPLY%20DETECT%20COMPLEX%20MALICIOUS%20QUERIES?%20A%20FRAMEWORK%20FOR%20JAILBREAKING%20VIA%20OBFUSCATING%20INTENT.pdf)
- 🔗 链接：<https://arxiv.org/abs/2405.03654>

17. 推动越狱攻击势头

- 📅 日期：2024-05-02
- 📑 文件：[Boosting Jailbreak Attack with Momentum](./Boosting%20Jailbreak%20Attack%20with%20Momentum.pdf)
- 🔗 链接：<https://arxiv.org/abs/2405.01229>

## 2024-04

16. 普遍的对抗触发因素并不具有普遍性

- 📅 日期：2024-04-24
- 📑 文件：[Universal Adversarial Triggers Are Not Universal](./Universal%20Adversarial%20Triggers%20Are%20Not%20Universal.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.16020>

15. 迭代提示多模态LLM以复制自然和AI生成的图像

- 📅 日期：2024-04-21
- 📑 文件：[Iteratively Prompting Multimodal LLMs to Reproduce Natural and AI-Generated Images](./Iteratively%20Prompting%20Multimodal%20LLMs%20to%20Reproduce%20Natural%20and%20AI-Generated%20Images.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.13784>

14. 大语言模型中的错误标记：分类法与有效检测方法

- 📅 日期：2024-04-19
- 📑 文件：[Glitch Tokens in Large Language Models: Categorization
Taxonomy and Effective Detection](./Glitch%20Tokens%20in%20Large%20Language%20Models-%20Categorization%20Taxonomy%20and%20Effective%20Detection.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.09894>

13. 指令层次结构：训练大语言模型优先处理特权指令

- 📅 日期：2024-04-19
- 📑 文件：[The Instruction Hierarchy:Training LLMs to Prioritize Privileged Instructions](./The%20Instruction%20Hierarchy-%20Training%20LLMs%20to%20Prioritize%20Privileged%20Instructions.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.13208>

12. 介绍来自MLCommons的人工智能安全基准v0.5

- 📅 日期：2024-04-18
- 📑 文件：[Introducing v0.5 of the AI Safety Benchmark from MLCommons](./Introducing%20v0.5%20of%20the%20AI%20Safety%20Benchmark%20from%20MLCommons.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.12241>

11. JailbreakLens：针对大型语言模型的越狱攻击可视化分析

- 📅 日期：2024-04-12
- 📑 文件：[JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models](./JailbreakLens-%20Visual%20Analysis%20of%20Jailbreak%20Attacks%20Against%20Large%20Language%20Models.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.08793>

10. 次毒性问题：深入探讨大型语言模型在越狱尝试中响应态度的变化

- 📅 日期：2024-04-12
- 📑 文件：[Subtoxic Questions: Dive Into Attitude Change of LLM's Response in Jailbreak Attempts](./Subtoxic%20Questions-%20Dive%20Into%20Attitude%20Change%20of%20LLM's%20Response%20in%20Jailbreak%20Attempts.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.08309>

9. AmpleGCG：学习通用且可转移的对抗后缀生成模型，用于破解开放和封闭的 LLM

- 📅 日期：2024-04-11
- 📑 文件：[AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs](./AmpleGCG-%20Learning%20a%20Universal%20and%20Transferable%20Generative%20Model%20of%20Adversarial%20Suffixes%20for%20Jailbreaking%20Both%20Open%20and%20Closed%20LLMs.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.07921>

8. AEGIS：使用大型语言模型专家团队进行在线自适应人工智能内容安全审核

- 📅 日期：2024-04-09
- 📑 文件：[AEGIS- Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts](./AEGIS-%20Online%20Adaptive%20AI%20Content%20Safety%20Moderation%20with%20Ensemble%20of%20LLM%20Experts.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.05993>

7. 目标引导的生成式提示注入攻击在大型语言模型上的应用

- 📅 日期：2024-04-06
- 📑 文件：[Goal-guided Generative Prompt Injection Attack on Large Language Models](./Goal-guided%20Generative%20Prompt%20Injection%20Attack%20on%20Large%20Language%20Models.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.07234>

6. 微调和量化增加了大型语言模型的漏洞

- 📅 日期：2024-04-05
- 📑 文件：[Increased LLM Vulnerabilities from Fine-tuning and Quantization](./Increased%20LLM%20Vulnerabilities%20from%20Fine-tuning%20and%20Quantization.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.04392>

5. 越狱提示攻击：一种可控的对扩散模型的对抗性攻击

- 📅 日期：2024-04-02
- 📑 文件：[Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models](./Jailbreaking%20Prompt%20Attack-%20A%20Controllable%20Adversarial%20Attack%20against%20Diffusion%20Models.pdf)
- 🔗 链接：<https://arxiv.org/abs/2404.02928>

## 2024-01

4. OWASP 大语言模型人工智能应用Top 10 安全威胁

- 📅 日期：2024-01-09
- 📑 文件：[OWASP 大语言模型人工智能应用Top 10 安全威胁](./OWASP%20大语言模型人工智能应用Top%2010%20安全威胁.pdf)
- 🔗 链接：<https://owasp.org/www-project-top-10-for-large-language-model-applications/>

## 2023-12 

3. 控制大型语言模型输出：入门

- 📅 日期：2023-12
- 📑 文件：[控制大型语言模型输出：入门](./CSET-Controlling-Large-Language-Model-Outputs-A-Primer.pdf)
- 🔗 链接：<https://cset.georgetown.edu/publication/controlling-large-language-models-a-primer/>


## 2023-11

2. 召唤恶魔并将其束缚：野外LLM红队攻击的实地理论

- 📅 日期：2023-11-10（最后修改日期：2023-11-13）
- 📑 文件：[Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild](./Summon%20a%20Demon%20and%20Bind%20it:%20A%20Grounded%20Theory%20of%20LLM%20Red%20Teaming%20in%20the%20Wild.pdf)
- 🔗 链接：<https://arxiv.org/abs/2311.06237>

## 2022-11

1. 忽略之前的提示：语言模型的攻击技术

- 📅 日期：2022-11-17
- 📑 文件：[Ignore Previous Prompt: Attack Techniques For Language Models](./Ignore%20Previous%20Prompt-%20Attack%20Techniques%20For%20Language%20Models.pdf)
- 🔗 链接：<https://arxiv.org/abs/2211.09527>
