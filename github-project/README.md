# Github 开源项目

1. ps-fuzz：强化生成式AI应用程序

- 🔗 链接：<https://github.com/prompt-security/ps-fuzz>
- 💬 简介：ps-fuzz 是一款交互式的系统提示词安全评估工具，它通过模拟攻击的方式来提供安全评估，使您能够根据需要加强系统提示词。ps-fuzz 还可以通过配置和域动态定制测试，并且提供了一个 Playground 聊天借口来迭代改进系统提示词，使其抵御更广泛的生成是人工智能攻击。

2. garak：LLM 漏洞扫描器

- 🔗 链接：<https://github.com/leondz/garak>
- 💬 简介：生成式AI的红队测试和评估工具包。garak 会测试幻觉、数据泄漏、提示注入、错误信息、生成有害内容、越狱以及其他许多弱点。如果你熟悉 nmap，它就像是 LLM 的 nmap。garak 是免费的工具，并且一直希望增加新功能以支持各种应用。

3. PromptInject

- 🔗 链接：<https://github.com/agencyenterprise/PromptInject>
- 💬 简介：PromptInject 项目提供了工具和示例，展示了如何使用不同的攻击方法来揭示语言模型中的安全漏洞。它旨在帮助研究人员和开发人员更好地理解这些风险，以便在构建和部署 LLM 时提高安全性。

4. LLMFuzzer：LLM 的模糊测试框架

- 🔗 链接：<https://github.com/mnns/LLMFuzzer>
- 💬 简介：LLMFuzzer是第一个专门为大型语言模型（LLM）设计的开源模糊框架，特别是基层了 LLM API。

5. PsychoEvals：用于大型语言模型的提示安全与心理测量框架。

- 🔗 链接：<https://github.com/NextWordDev/psychoevals>
- 💬 简介：PsychoEvals 是一个轻量级的Python库，用于评估和确保大型语言模型（LLMs）和代理的行为，例如OpenAI的GPT系列。这个库提供了一个测试框架，帮助研究人员、开发者和爱好者通过心理测量测试、安全特性和审核工具，更好地理解、评估和确保LLMs的安全。

6. Rebuff：自加固的提示词注入探测器

- 🔗 链接：<https://github.com/protectai/rebuff>
- 💬 简介：Rebuff是Protect AI于2023年7月获得的一个开源项目，目前仍在持续开发中。该项目主要解决的是提示注入（Prompt Injection，简称PI）攻击问题。提示注入攻击是指攻击者向大型语言模型（Large Language Models，简称LLM）发送恶意输入，以篡改输出、暴露敏感数据或进行未授权操作。Rebuff作为一个自我强化的提示注入检测器，能够在遭受攻击时变得更强，以此来保护AI应用免受这类攻击。它通过多阶段防御机制来实现这一目标，目前仍处于alpha测试阶段。

7. AI Goat：AI安全学习平台，本地运行的CTF挑战

- 🔗 链接：<https://github.com/dhammon/ai-goat>
- 💬 简介：“AI Goat” 是一个开源项目，旨在通过一系列易受攻击的大型语言模型（LLM）的CTF挑战来教授AI安全。该项目不需要注册或云费用，所有内容都可以在本地系统上运行。AI Goat的开发是为了学习LLM的开发和安全风险，特别是在公司中使用LLM时所面临的风险。这些挑战的CTF格式是安全研究人员获得实际经验并了解这些系统如何易受攻击和被利用的好方法

8. PromptBench：用于评估和理解大型语言模型的统一库

- 🔗 链接：<https://github.com/microsoft/promptbench>
- 💬 简介：“PromptBench” 是一个基于PyTorch的Python库，用于评估和理解大型语言模型（LLMs）。它为研究人员提供了用户友好的API，用于对LLMs进行评估。PromptBench支持快速模型性能评估、提示工程、评估对抗性提示和动态评估等多种功能。此外，它还提供了不同的教程，帮助用户在现有基准上评估模型、测试不同提示技术的效果、检查提示攻击的稳健性以及使用DyVal进行评估。目前，PromptBench支持多种数据集、模型、提示工程方法、对抗性攻击等，并欢迎用户添加更多内容

9. PyRIT：用于生成人工智能的Python风险识别工具

- 🔗 链接：<https://github.com/Azure/PyRIT>
- 💬 简介：“PyRIT”（Python Risk Identification Tool for generative AI）是一个开源的自动化框架，旨在帮助安全专业人员和机器学习工程师主动发现他们的大型语言模型（LLM）系统中的风险。这个工具由微软的AI红队开发，旨在帮助研究人员和工程师评估他们的LLM端点在面对不同伤害类别（如制造/无根据的内容、滥用和禁止内容）时的稳健性。PyRIT自动化AI红队任务，使操作者能够专注于更复杂和耗时的任务，并能够识别诸如滥用（例如，恶意软件生成、越狱）和隐私伤害（例如，身份盗窃）等安全伤害。其目标是让研究人员能够有一个基线，了解他们的模型及其整个推理管道在不同的伤害类别中的表现，并能够将这个基线与未来模型的迭代进行比较。这使他们能够拥有关于他们的模型当前表现的实际数据，并检测基于未来改进的任何性能下降.

10. AmpleGCG: 学习一个既适用于开放也适用于封闭语言模型（LLM）的通用且可迁移的对抗性攻击生成器

- 🔗 链接：<https://github.com/OSU-NLP-Group/AmpleGCG>
- 💬 简介：OSU-NLP-Group的AmpleGCG项目是一个研究项目，旨在学习一个通用且可迁移的对抗性攻击生成器，用于攻击开放和封闭的语言模型（LLM）。这个项目扩展了GCG（生成对抗性攻击的模型）的功能，通过过生成样本来提高攻击成功率，更全面地识别漏洞，并提高在开源和闭源模型上的效率。由于安全和伦理考虑，该项目没有公开释放训练好的AmpleGCG模型，但可以通过申请获取，仅用于研究目的。

10. 使用动量攻击时，有哪些需要注意的伦理问题？

- 🔗 链接：<https://github.com/weizeming/momentum-attack-llm>
- 💬 简介：关于提升针对语言模型（LLM）的"越狱攻击"的研究。这个项目使用动量来增强攻击效果，是ICLR 2024关于可靠和负责任的基模型研讨会上的官方实现。目前，这个项目还在建设中。
